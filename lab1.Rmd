---
title: "Lab 1: NYT API"
author: "Andrew Bartnik"
date: "2023-04-05"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(jsonlite) #convert results from API queries into R-friendly formats 
library(plyr)
library(tidyverse)
library(tidytext) #text data management and analysis
library(ggplot2) #plot word frequencies and publication dates
#assign API key.  When you create a NYT Dev account, you will be given a key
API_KEY <- "9GwTALiMoXG8etvjlNqbvwzZVwKcWrua"
```

```{r}
#create the query url
url <- paste("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=haaland&api-key=",API_KEY, sep ="")
#send the request, receive the response, and flatten
t <- fromJSON(url, flatten = T)
```

## Assignment (Due Tuesday 4/11 11:59pm)

1.  Create a free New York Times account (<https://developer.nytimes.com/get-started>)

2.  Pick an interesting environmental key word(s) and use the {jsonlite} package to query the API. Pick something high profile enough and over a large enough time frame that your query yields enough articles for an interesting examination.

```{r}
term2 <- "biodiversity" # Need to use $ to string  together separate terms
begin_date2 <- "20210120"
end_date2 <- "20230401"
#construct the query url using API operators
baseurl2 <- paste0("http://api.nytimes.com/svc/search/v2/articlesearch.json?q=",term2,"&begin_date=",begin_date2,"&end_date=",end_date2,"&facet_filter=true&api-key=","NTKBHbsb6XFEkGymGumAiba7n3uBvs8V", sep="")
#examine our query url
baseurl2
```
```{r}
#dig into the JSON object to find total hits
initialQuery2 <- fromJSON(baseurl2)
maxPages <- round((initialQuery2$response$meta$hits[1] / 20)-1) 
#initiate a list to hold results of our for loop
pages2 <- list()
#loop
for(i in 0:maxPages){
  nytSearch2 <- fromJSON(paste0(baseurl2, "&page=", i), flatten = TRUE) %>% data.frame() 
  message("Retrieving page ", i)
  pages2[[i+1]] <- nytSearch2 
  Sys.sleep(60) 
  }
```

```{r}
#need to bind the pages and create a tibble from nytDat
df2 <- rbind.fill(pages2)
saveRDS(df2, "nyt_df2.rds")

```

```{r}
nytDat2 <- readRDS("nyt_df2.rds")
dim(nytDat2)
df2 <- nytDat2
```





3.  Recreate the publications per day and word frequency plots using the first paragraph.  This time filter on the response.docs.news_desk variable to winnow out irrelevant results.
-   Make some (at least 3) transformations to the corpus (add context-specific stopword(s), stem a key term and its variants, remove numbers)




## Publications per day

```{r}
paragraph2 <- names(nytDat2)[6] 
tokenized2 <- nytDat2 %>%
unnest_tokens(word, paragraph2) 

# Remove numbers
clean_tokens2 <- str_remove_all(tokenized2$word, "[:digit:]")

# Remove apostrophes 
clean_tokens2 <- gsub("â€™s", '', clean_tokens2)
tokenized2$clean <- clean_tokens2

#remove the empty strings
tib2 <-subset(tokenized2, clean!="")
#reassign
tokenized2 <- tib2
```


```{r}
ppd <- tokenized2 %>%
  mutate(pubDay=gsub("T.*","",response.docs.pub_date)) %>%
  filter(response.docs.news_desk %in% c("Climate", "Science")) |> 
  group_by(pubDay) %>%
  summarise(count=n()) %>%
  filter(count >= 50) %>%
  ggplot() +
  geom_bar(aes(x=reorder(pubDay, count), y=count), stat="identity") +
  coord_flip()
ppd
```



## Word frequency

```{r}
data(stop_words)
stop_words
tokenized2 <- tokenized2 %>%
  anti_join(stop_words)
wf <- tokenized2 %>%
  count(clean, sort = TRUE) %>%
  filter(n > 10) %>%
  mutate(clean = reorder(clean, n)) %>%
  ggplot(aes(n, clean)) +
  geom_col() +
  labs(y = NULL)

wf
```



4.  Recreate the publications per day and word frequency plots using the headlines variable (response.docs.headline.main). Compare the distributions of word frequencies between the first paragraph and headlines. Do you see any difference?

# Publications per day
```{r}
headline <- names(nytDat2)[20] 
token_headlines <- nytDat2 %>%
unnest_tokens(word, headline) 
head(token_headlines[,"word"])
head(token_headlines$word)

#remove the empty strings
tib3 <-subset(token_headlines, word!="")
#reassign
token_headlines <- tib3
```

```{r}
data(stop_words)
stop_words
token_headlines <- token_headlines %>%
  anti_join(stop_words)

ppd2 <- token_headlines %>%
  mutate(pubDay=gsub("T.*","",response.docs.pub_date)) %>%
  filter(response.docs.news_desk %in% c("Climate", "Science")) |> 
  group_by(pubDay) %>%
  summarise(count=n()) %>%
  filter(count >= 8) %>%
  ggplot() +
  geom_bar(aes(x=reorder(pubDay, count), y=count), stat="identity") +
  coord_flip()
```

# Word frequency
```{r}
#reassign
token_headlines <- tib3
wf2 <- token_headlines %>%
  count(word, sort = TRUE) %>%
  filter(n > 5) %>%
  mutate(word = reorder(word, n)) %>%
  ggplot(aes(n, word)) +
  geom_col() +
  labs(y = NULL)
```



```{r}
gridExtra::grid.arrange(wf, wf2, ncol=2)
```
**The word frequencies in the headlines and first paragraphs within NYT "biodiversity" articles are very similar. Virtually all of the words that appear in the headlines also appear in the first paragraph. 

