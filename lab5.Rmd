---
title: "Lab5"
author: "Andrew Bartnik"
date: "2023-05-10"
output: html_document
---

```{r packages, include = FALSE}
library(tidytext)
library(tidyverse)
library(widyr)
library(irlba) 
library(broom) 
library(textdata)
library(ggplot2)
library(dplyr)
library(LexisNexisTools)
```

We'll start off today by loading the climbing incident data again.

```{r data,}
incidents_df<-read_csv("https://raw.githubusercontent.com/MaRo406/EDS-231-text-sentiment/main/data/climbing_reports_model_dat.csv")
```

First, let's calculate the unigram probabilities -- how often we see each word in this corpus.

```{r unigrams}
unigram_probs <- incidents_df |> unnest_tokens(word, Text) |> anti_join(stop_words, by = 'word') |> count(word, sort=T) |> mutate(p = n/sum(n))
unigram_probs
```

OK, so that tells us the probability of each word.

Next, we need to know how often we find each word near each other word -- the skipgram probabilities. In this case we'll define the word context as a five-word window. We'll slide that window across all of our text and record which words occur together within that window.

Now let's write some code that adds an ngramID column that contains constituent information about each 5-gram we constructed by sliding our window.

```{r make-skipgrams}
skipgrams <- incidents_df |> unnest_tokens(ngram, Text, token = "ngrams", n = 5) |> mutate(ngramID = row_number()) |> tidyr::unite(skipgramID, ID, ngramID) |> unnest_tokens(word, ngram) |> anti_join(stop_words, by = 'word')
```

Now we use widyr::pairwise_count() to sum the total \# of occurences of each pair of words.

```{r pairwise_count}
#calculate probabilities
skipgram_probs <- skipgrams |> pairwise_count(word, skipgramID,diag = T, sort = T) |> mutate(p = n/sum(n))
```

The next step is to normalize these probabilities, that is, to calculate how often words occur together within a window, relative to their total occurrences in the data.

```{r norm-prob}
normalized_prob <- skipgram_probs |> filter(n>20) |> rename(word1 = item1, word2=item2) |> left_join(unigram_probs |> select(word1 = word, p1 = p), by = 'word1') |> 
  left_join(unigram_probs |> select(word2 = word, p2 = p), by = 'word2') |> mutate(p_together = p/p1/p2)

normalized_prob
```

Now we have all the pieces to calculate the point-wise mutual information (PMI) measure. It's the logarithm of the normalized probability of finding two words together. PMI tells us which words occur together more often than expected based on how often they occurred on their own.

Then we convert to a matrix so we can use matrix factorization and reduce the dimensionality of the data.

```{r pmi}
pmi_matrix <- normalized_prob |> mutate(pmi = log10(p_together)) |> cast_sparse(word1, word2, pmi)
```

We do the singluar value decomposition with irlba::irlba(). It's a "partial decomposition" as we are specifying a limited number of dimensions, in this case 100.

```{r pmi2}
pmi_matrix@x[is.na(pmi_matrix@x)]<0
pmi_svd <- irlba(pmi_matrix, 100, maxit = 500)
word_vectors <- pmi_svd$u

rownames(word_vectors) <- rownames(pmi_matrix)
```

These vectors in the "u" matrix are contain "left singular values". They are orthogonal vectors that create a 100-dimensional semantic space where we can locate each word. The distance between words in this space gives an estimate of their semantic similarity.

Here's a function written by Julia Silge for matching the most similar vectors to a given vector.

```{r syn-function}
search_synonyms <- function(word_vectors, selected_vector) {
dat <- word_vectors %*% selected_vector
    
similarities <- dat %>%
        tibble(token = rownames(dat), similarity = dat[,1])

similarities %>%
       arrange(-similarity) %>%
        select(c(2,3))
}
```

Let's test it out!

```{r find-synonyms}
fall <- search_synonyms(word_vectors = word_vectors, word_vectors['fall',])
fall

slip <- search_synonyms(word_vectors = word_vectors, word_vectors['slip',])
slip

ice <- search_synonyms(word_vectors = word_vectors, word_vectors['ice',])
ice
```

Here's a plot for visualizing the most similar words to a given target word.

```{r plot-synonyms}
slip %>%
    mutate(selected = "slip") %>%
    bind_rows(fall %>%
                  mutate(selected = "fall")) %>%
    group_by(selected) %>%
    top_n(15, similarity) %>%
    ungroup %>%
    mutate(token = reorder(token, similarity)) %>%
    ggplot(aes(token, similarity, fill = selected)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~selected, scales = "free") +
    coord_flip() +
    theme(strip.text=element_text(hjust=0, size=12)) +
    scale_y_continuous(expand = c(0,0)) +
    labs(x = NULL, title = "What word vectors are most similar to slip or fall?")
  
```

One of the cool things about representing words as numerical vectors is that we can use math on those numbers that has some semantic meaning.

```{r word-math}
snow_danger <- word_vectors["snow",] + word_vectors["danger",]
search_synonyms(word_vectors, snow_danger)

no_snow_danger <- word_vectors["danger",] - word_vectors["snow",]
search_synonyms(word_vectors, no_snow_danger)
```

### Assignment

#### Train Your Own Embeddings

#1.  
Using the data from your Nexis Uni query from Week 2, create a set of word embeddings. To do this, you'll essentially need to recreate the steps in today's example down through the chunk named "pmi".

```{r}
pre_files <- list.files(pattern = ".docx", 
                        path = "~/Desktop/MEDS/Spring/text/text_analysis/lab2/files2",
                       full.names = TRUE, 
                       recursive = TRUE, 
                       ignore.case = TRUE)


pre_dat <- lnt_read(pre_files)
```

```{r}
meta <- pre_dat@meta
articles <- pre_dat@articles
paragraphs <- pre_dat@paragraphs

data <- tibble(Date = meta$Date, Headline = meta$Headline, id = pre_dat@articles$ID, text = pre_dat@articles$Article)
```


```{r}
unigram_probs_lexis <- data |> unnest_tokens(word, text) |> anti_join(stop_words, by = 'word') |> count(word, sort=T) |> mutate(p = n/sum(n))
unigram_probs_lexis

```
```{r}
lexis_skipgrams <- data |> unnest_tokens(ngram, text, token = "ngrams", n = 5) |> mutate(ngramID = row_number()) |> tidyr::unite(skipgramID, id, ngramID) |> unnest_tokens(word, ngram) |> anti_join(stop_words, by = 'word')
```


```{r pairwise_count2}
#calculate probabilities
lexis_skipgram_probs <- lexis_skipgrams |> pairwise_count(word, skipgramID,diag = T, sort = T) |> mutate(p = n/sum(n))
```


```{r norm-prob2}
norm_prob_lexis <- lexis_skipgram_probs |> 
  filter(n>20) |> 
  rename(word1 = item1, word2=item2) |> 
  left_join(unigram_probs_lexis |> select(word1 = word, p1 = p), by = 'word1') |> 
  left_join(unigram_probs_lexis |> select(word2 = word, p2 = p), by = 'word2') |> mutate(p_together = p/p1/p2)
```

```{r}
pmi_matrix_lexis <- norm_prob_lexis |> mutate(pmi = log10(p_together)) |> cast_sparse(word1, word2, pmi)
pmi_matrix_lexis@x[is.na(pmi_matrix_lexis@x)]<0
pmi_svd <- irlba(pmi_matrix_lexis, 100, maxit = 500)
word_vectors_lexis <- pmi_svd$u

rownames(word_vectors_lexis) <- rownames(pmi_matrix_lexis)
```


#2.  
Think of 3-5 key words in your data set. Calculate and plot the 10 most semantically similar words for each of them. Identify and interpret any interesting or surprising results.

**biodiversity, climate, species, nature, and change**
```{r}
biodiversity <- search_synonyms(word_vectors = word_vectors_lexis, word_vectors_lexis['biodiversity',])
biodiversity

climate <- search_synonyms(word_vectors = word_vectors_lexis, word_vectors_lexis['climate',])
climate

species <- search_synonyms(word_vectors = word_vectors_lexis, word_vectors_lexis["species",])
species

nature <- search_synonyms(word_vectors = word_vectors_lexis, word_vectors_lexis['nature',])
nature

change <- search_synonyms(word_vectors = word_vectors_lexis, word_vectors_lexis["change",])
change

```



```{r}
biodiversity %>%
    mutate(selected = "biodiversity") %>%
    bind_rows(species %>%
                  mutate(selected = "species"),
              climate |> mutate(selected = "climate"),
              nature |> mutate(selected = "nature"),
              change |> mutate(selected = "change")) %>%
    group_by(selected) %>%
    top_n(15, similarity) %>%
    ungroup %>%
    mutate(token = reorder(token, similarity)) %>%
    ggplot(aes(token, similarity, fill = selected)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~selected, scales = "free") +
    coord_flip() +
    theme(strip.text=element_text(hjust=0, size=12)) +
    scale_y_continuous(expand = c(0,0)) +
    labs(x = NULL, title = "Word vectors similar to biodiversity, species, climate, nature, and change ")
```



# 3.  
Assemble 3 word math equations that you think could be useful or interesting for exploring the meaning of key words or ideas in your data set.
**Climate crisis, species not extinct, biodiversity hotspots**

```{r}
climate_crisis <- word_vectors_lexis["climate",] + word_vectors_lexis["crisis",]
search_synonyms(word_vectors_lexis, climate_crisis)

no_extinction <- word_vectors_lexis["extinction",] - word_vectors_lexis["species",]
search_synonyms(word_vectors_lexis, no_extinction)

biodiversity_hotspot <- word_vectors_lexis["biodiversity",] + word_vectors_lexis["hotspots",]
search_synonyms(word_vectors_lexis, biodiversity_hotspot)
```


#4.  
Following the example in the SMLTR text (section 5.4), create a set of 100-dimensional GloVe word embeddings. These embeddings were trained by researchers at Stanford on 6 billion tokens from Wikipedia entries.

```{r}
glove6b <- read_csv("glove6b.csv")

# Convert the data frame to a matrix
glove6b_matrix <- as.matrix(glove6b[,-(1:2)]) 

# Set the row names of the matrix to be the token column from the data frame
rownames(glove6b_matrix) <- glove6b$token
```






#### Pretrained Embeddings


#5.  
Test them out with the cannonical word math equation on the GloVe embeddings: "berlin" - "germany" + "france" = ?


```{r}

glove_math <- glove6b_matrix["berlin",] - glove6b_matrix["germany",] + glove6b_matrix["france",]
search_synonyms(glove6b_matrix, glove_math)
```


#6.  
Recreate parts 2 and 3 above using the the GloVe embeddings in place of the ones you trained. How do they compare? What are the implications for applications of these embeddings?

**The synonym similarities for the GloVe embeddings are much higher than for the articles I selected. The synonyms chosen are also much more pertinent to each word. This is expected since the GloVe embeddings are much more comprehensive.**
```{r}
biodiversity <- search_synonyms(word_vectors = glove6b_matrix, glove6b_matrix['biodiversity',])
biodiversity

climate <- search_synonyms(word_vectors = glove6b_matrix, glove6b_matrix['climate',])
climate

species <- search_synonyms(word_vectors = glove6b_matrix, glove6b_matrix["species",])
species

nature <- search_synonyms(word_vectors = glove6b_matrix, glove6b_matrix['nature',])
nature

change <- search_synonyms(word_vectors = glove6b_matrix, glove6b_matrix["change",])
change
```

```{r}
biodiversity %>%
    mutate(selected = "biodiversity") %>%
    bind_rows(species %>%
                  mutate(selected = "species"),
              climate |> mutate(selected = "climate"),
              nature |> mutate(selected = "nature"),
              change |> mutate(selected = "change")) %>%
    group_by(selected) %>%
    top_n(15, similarity) %>%
    ungroup %>%
    mutate(token = reorder(token, similarity)) %>%
    ggplot(aes(token, similarity, fill = selected)) +
    geom_col(show.legend = FALSE) +
    facet_wrap(~selected, scales = "free") +
    coord_flip() +
    theme(strip.text=element_text(hjust=0, size=12)) +
    scale_y_continuous(expand = c(0,0)) +
    labs(x = NULL, title = "Word vectors similar to biodiversity, species, climate, nature, and change ")
```
 
```{r}
climate_crisis <- glove6b_matrix["climate",] + glove6b_matrix["crisis",]
search_synonyms(glove6b_matrix, climate_crisis)

no_extinction <- glove6b_matrix["extinction",] - glove6b_matrix["species",]
search_synonyms(glove6b_matrix, no_extinction)

biodiversity_hotspot <- glove6b_matrix["biodiversity",] + glove6b_matrix["hotspots",]
search_synonyms(glove6b_matrix, biodiversity_hotspot)
```

